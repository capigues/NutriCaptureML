{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3965ba81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import check_random_state\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import imageio.v2 as imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6a0339",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load Classnames\n",
    "path='/Users/cap/Dropbox/Cuisines'\n",
    "img_path='/Users/cap/Dropbox/Cuisines/images'\n",
    "classes = []\n",
    "num_foods = 0\n",
    "img_count = 0\n",
    "\n",
    "for file in os.listdir(img_path):\n",
    "    if (file.startswith('.')): continue #ignore hidden files\n",
    "    classes.append(file)\n",
    "\n",
    "num_foods = len(classes)\n",
    "    \n",
    "'''\n",
    "# More legible food names\n",
    "for i in range (0, num_foods-1):\n",
    "    if (dir_list[i].startswith('.')): continue #ignore hidden files\n",
    "    class_name = ''\n",
    "    for char in dir_list[i]:\n",
    "        if (char == '-'):\n",
    "            class_name += \" \"\n",
    "        else:\n",
    "            class_name += char\n",
    "    classes.append(class_name.title())\n",
    "'''\n",
    "\n",
    "food_names = sorted(classes)\n",
    "food_dict = {\n",
    "    1:'bangbang-chicken',\n",
    "    2:'dan-dan-noodles',\n",
    "    3: 'sichuan-hot-pot',\n",
    "    4: 'twice-cooked-pork',\n",
    "    5: 'wontons-in-chili-oil',\n",
    "}\n",
    "print(food_names, num_foods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c030f872",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Images using imageio and os package\n",
    "\n",
    "base_dir = '/Users/cap/Dropbox/Cuisines/images'\n",
    "\n",
    "# list all images in each file\n",
    "# in each file split dataset into train and test\n",
    "# add train and test file name to dictionary\n",
    "\n",
    "images_dict = {\n",
    "}\n",
    "\n",
    "for i in range(0, num_foods):\n",
    "    name = food_names[i]\n",
    "    images_dict[name] = []\n",
    "    for img in os.listdir(os.path.join(img_path,name)):\n",
    "        if (img.startswith('.')): continue #ignore hidden files\n",
    "        images_dict[name].append(img)\n",
    "\n",
    "\n",
    "dataset = {}\n",
    "\n",
    "for class_id in images_dict:\n",
    "    images_list = images_dict[class_id]\n",
    "    train_stop = int(len(images_list) * 0.8)\n",
    "    train_files = images_list[:train_stop]\n",
    "    validation_files = images_list[train_stop:]\n",
    "    train_images = [imageio.imread(os.path.join(img_path,os.path.join(class_id,f))) for f in train_files]\n",
    "    validation_images = [imageio.imread(os.path.join(img_path, os.path.join(class_id,f))) for f in validation_files]\n",
    "    dataset[class_id] = {\n",
    "        'train': train_images,\n",
    "        'val': validation_images,\n",
    "        'train_f': train_files,\n",
    "        'val_f': validation_files\n",
    "    }\n",
    "\n",
    "for class_id in dataset:\n",
    "    img_count += len(dataset[class_id]['train_f']) + len(dataset[class_id]['val_f'])\n",
    "    print('train: {}'.format(class_id), dataset[class_id]['train_f'], sep='\\n')\n",
    "    print('test: {}'.format(class_id), dataset[class_id]['val_f'], sep='\\n')\n",
    "    print('\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856546d2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import random\n",
    "import skimage.transform as transforms\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "class FeatureExtractor:\n",
    "    def __init__(self, in_features=20, out_classes=num_foods):\n",
    "        self.pca = PCA(n_components = in_features)\n",
    "        self.out_classes = out_classes\n",
    "        self.training = True\n",
    "    \n",
    "    def extract_features(self, x):\n",
    "        self.pca.fit((x[:,:,0] + x[:,:,1] + x[:,:,2])/3)\n",
    "        p = self.pca.singular_values_\n",
    "        return p\n",
    "        \n",
    "        \n",
    "class SimpleDataloader:\n",
    "    def __init__(self, images_dict, shuffle=False, width=224, height=224):\n",
    "        self.feature_extractor = FeatureExtractor()\n",
    "        self.images_dict = images_dict\n",
    "        self.mode = 'train'\n",
    "        self.train_list = []\n",
    "        self.val_list = []\n",
    "        self.shuffle = shuffle\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.train_images = []\n",
    "        self.val_images = []\n",
    "        self.train_features = []\n",
    "        self.val_features = []\n",
    "        pbar = tqdm(total=img_count)\n",
    "        for class_id in self.images_dict:\n",
    "            for i in range(len(self.images_dict[class_id]['train'])):\n",
    "                self.train_list.append((class_id, i))\n",
    "                image = self.transform(self.images_dict[class_id]['train'][i])\n",
    "                features = self.feature_extractor.extract_features(image)\n",
    "                self.train_images.append(image)\n",
    "                self.train_features.append(features)\n",
    "                pbar.update(1)\n",
    "            for i in range(len(self.images_dict[class_id]['val'])):\n",
    "                self.val_list.append((class_id,i))\n",
    "                image = self.transform(self.images_dict[class_id]['val'][i])\n",
    "                features = self.feature_extractor.extract_features(image)\n",
    "                self.val_images.append(image)\n",
    "                self.val_features.append(features)\n",
    "                pbar.update(1)\n",
    "            \n",
    "        self.set_mode('train', True)\n",
    "        \n",
    "    def set_shuffle(self, shuffle):\n",
    "        self.shuffle = shuffle\n",
    "        \n",
    "        \n",
    "    def reset_shuffle(self):\n",
    "        self.indexes = list(range(len(self.data_list)))\n",
    "        if self.shuffle:\n",
    "            random.shuffle(self.indexes)\n",
    "    \n",
    "    def set_mode(self, mode, shuffle):\n",
    "        self.set_shuffle(shuffle)\n",
    "        assert mode == 'train' or mode == 'val', 'only supports training or validation'\n",
    "        self.mode = mode\n",
    "        if mode == 'train':\n",
    "            self.data_list = self.train_list\n",
    "            self.data = self.train_images\n",
    "            self.features = self.train_features\n",
    "        else:\n",
    "            self.data_list = self.val_list\n",
    "            self.data = self.val_images\n",
    "            self.features = self.val_features\n",
    "        self.reset_shuffle()\n",
    "        \n",
    "    def transform(self, image):\n",
    "        height, width, ch = image.shape\n",
    "        if width < height:\n",
    "            new_width = self.width\n",
    "            scale = self.width / width\n",
    "            new_height = int(height * scale)\n",
    "        else:\n",
    "            new_height = self.height\n",
    "            scale = self.height / height\n",
    "            new_width = int(width * scale)\n",
    "        image_tf = transforms.resize(image, (new_height, new_width))\n",
    "        \n",
    "        # center crop\n",
    "        w_start = 0\n",
    "        w_stop = self.width\n",
    "        h_start = 0\n",
    "        h_stop = self.height\n",
    "        if new_width > self.width:\n",
    "            start = (new_width - self.width) // 2\n",
    "            w_start = start\n",
    "            w_stop = start + self.width\n",
    "        if new_height > self.height:\n",
    "            start = (new_height - self.height) // 2\n",
    "            h_start = start\n",
    "            h_stop = start + self.height\n",
    "        image_tf = image_tf[h_start:h_stop, w_start:w_stop, :]\n",
    "        return image_tf\n",
    "    \n",
    "    def __getitem__(self, data_index):\n",
    "        i = self.indexes[data_index]\n",
    "        class_id, idx = self.data_list[i]\n",
    "        if data_index == len(self.data_list):\n",
    "            self.reset_shuffle\n",
    "        return self.data[i], self.features[i], class_id\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "                   \n",
    "food_dataloader = SimpleDataloader(dataset,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7666ef6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "food_dataloader.set_mode('train', True) \n",
    "\n",
    "imgs = []\n",
    "count = 0\n",
    "for i, (image, features, class_id) in enumerate((food_dataloader)):\n",
    "#     print(class_id) # We expect the first class id to be random when shuffle is true\n",
    "                  # Here is where we would run whatever training iteration we wanted.\n",
    "    imgs.append(image)\n",
    "    count += 1\n",
    "    if count >= 6:\n",
    "        break\n",
    "        \n",
    "example_images = np.concatenate([np.concatenate(imgs[:3], axis=1), np.concatenate(imgs[3:], axis=1)], axis=0)\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "ax.imshow(example_images)\n",
    "ax.axis('off')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d81e667",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "def save_model(classifier):\n",
    "    filename = '/Users/cap/Dropbox/Cuisines/Models/NutriCaptureML_' + classifier.algorithm\n",
    "    pickle.dump(classifier, open(filename, 'wb'))\n",
    "\n",
    "def gather_features_foods(all_classes, mode, shuffle):\n",
    "    #Load preprocessed features from dataloader\n",
    "    food_dataloader.set_mode(mode, shuffle)\n",
    "    image_features = []\n",
    "    classes = []\n",
    "    for j, (image, features, class_id) in enumerate(food_dataloader):\n",
    "        if class_id not in all_classes:\n",
    "            continue\n",
    "        classes.append(list(all_classes).index(class_id))\n",
    "        image_features.append(features)\n",
    "    scaler = StandardScaler()\n",
    "    classes = np.array(classes)\n",
    "    image_features = np.stack(image_features, axis=0)\n",
    "    return image_features, classes\n",
    "\n",
    "def train(classifier, all_classes=None, quiet=False, return_lda=False):\n",
    "    if not quiet:\n",
    "        print(\"TRAINING FOODS\")\n",
    "    train_features, train_classes = gather_features_foods(all_classes, 'train', True)\n",
    "    # train LDA and train classifier\n",
    "    lda = LinearDiscriminantAnalysis(n_components=1).fit(train_features, train_classes)\n",
    "    train_features = lda.transform(train_features)\n",
    "    classifier.fit(train_features, train_classes)\n",
    "    \n",
    "#     save_model(classifier)\n",
    "    \n",
    "    # score train classification\n",
    "    train_acc = classifier.score(train_features, train_classes)\n",
    "    \n",
    "    #load validation data\n",
    "    val_features, val_classes = gather_features_foods(all_classes, 'val', False)\n",
    "    \n",
    "    # transform LDA\n",
    "    val_features = lda.transform(val_features)\n",
    "    # score validation classification\n",
    "    val_acc = classifier.score(val_features, val_classes)\n",
    "    \n",
    "    if return_lda:\n",
    "        return train_acc, val_acc, lda\n",
    "    return train_acc, val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63503bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_classes = np.random.choice(range(1,6), 2, replace=False)\n",
    "features = 5\n",
    "depth = 18\n",
    "estimators = 15\n",
    "classifier = AdaBoostClassifier(\n",
    "    DecisionTreeClassifier(max_depth=depth),\n",
    "    n_estimators=estimators,\n",
    "    learning_rate=1.5,\n",
    "    algorithm=\"SAMME\"\n",
    ")\n",
    "\n",
    "model_filename = 'NutriCaptureML'\n",
    "pickle.dump\n",
    "\n",
    "new_classes = np.array([food_dict[all_classes[0]],food_dict[all_classes[1]]], \"str\", ndmin=1)\n",
    "t_acc, v_acc = train(classifier, new_classes)\n",
    "\n",
    "\n",
    "print(\"train_acc {} val acc {}, random {}\".format(t_acc, v_acc, 1./5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43bb026e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "all_classes = np.random.choice(range(1, 6), 2, replace=False)\n",
    "classifier = LogisticRegression(\n",
    "                C=1000. / len(food_dataloader), penalty='l2', solver='saga', tol=0.01\n",
    "             )\n",
    "new_classes = np.array([food_dict[all_classes[0]],food_dict[all_classes[1]]], \"str\", ndmin=1)\n",
    "t_acc, v_acc = train(classifier, new_classes)\n",
    "print(\"train_acc {} val acc {}, random {}\".format(t_acc, v_acc, 1./5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3ee531",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "classifier = MLPClassifier(solver='lbfgs', alpha=1e-5,\n",
    "                    hidden_layer_sizes=(15, 15), random_state=1, max_iter=10000, warm_start=True)\n",
    "new_classes = np.array([food_dict[all_classes[0]],food_dict[all_classes[1]]], \"str\", ndmin=1)\n",
    "t_acc, v_acc = train(classifier, new_classes)\n",
    "print(\"train_acc {} val acc {}, random {}\".format(t_acc, v_acc, 1./5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a093fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ada_classifiers = []\n",
    "t_accs, v_accs = [], []\n",
    "random_acc = [1./N for N in range(2, 6)]\n",
    "for c in range(2, 6):\n",
    "    added = False\n",
    "    best = ()\n",
    "    best_v = 0\n",
    "    for depth in range(2, 18, 2):\n",
    "        for estimators in range(3, 15, 3):\n",
    "            if c == 5:\n",
    "                all_classes = np.array(range(1, 6))\n",
    "            else:\n",
    "                all_classes = np.random.choice(range(1, 6), c, replace=False)\n",
    "            classifier = AdaBoostClassifier(\n",
    "                DecisionTreeClassifier(max_depth=depth),\n",
    "                n_estimators=estimators,\n",
    "                learning_rate=1.5,\n",
    "                algorithm=\"SAMME\"\n",
    "            )\n",
    "            new_classes = np.array([food_dict[all_classes[0]],food_dict[all_classes[1]]], \"str\", ndmin=1)\n",
    "            t_acc, v_acc, lda = train(classifier, new_classes, True, True)\n",
    "            if v_acc > best_v:\n",
    "                best = (t_acc, v_acc)\n",
    "                if added:\n",
    "                    ada_classifiers[-1] = (classifier, lda)\n",
    "                else:\n",
    "                    ada_classifiers.append((classifier, lda))\n",
    "                    added = True\n",
    "    t_accs.append(best[0])\n",
    "    v_accs.append(best[1])\n",
    "    print(\"{}: train acc {} val acc {} random {}\".format(c, best[0], best[1], 1./c))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11aaf533",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "mlp_classifiers = []\n",
    "t_accs_mlp, v_accs_mlp = [], []\n",
    "random_acc = [1./N for N in range(2, 6)]\n",
    "for c in range(2, 6):\n",
    "    added = False\n",
    "    for hidden in range(5, 20, 5):\n",
    "        best = ()\n",
    "        best_v = 0\n",
    "        if c == 5:\n",
    "            all_classes = np.array(range(1, 6))\n",
    "        else:\n",
    "            all_classes = np.random.choice(range(1, 6), c, replace=False)\n",
    "        features = 5\n",
    "        depth = 18\n",
    "        estimators = 15\n",
    "        classifier =  MLPClassifier(solver='lbfgs', alpha=1e-5,\n",
    "                hidden_layer_sizes=(hidden, hidden), random_state=1,\n",
    "                max_iter=10000, warm_start=True, early_stopping=True\n",
    "        )\n",
    "        new_classes = np.array([food_dict[all_classes[0]],food_dict[all_classes[1]]], \"str\", ndmin=1)\n",
    "        t_acc, v_acc, lda = train(classifier, new_classes, True, True)\n",
    "        if v_acc > best_v:\n",
    "            best = (t_acc, v_acc)\n",
    "            if added:\n",
    "                mlp_classifiers[-1] = (classifier, lda)\n",
    "            else:\n",
    "                mlp_classifiers.append((classifier, lda))\n",
    "    t_accs_mlp.append(best[0])\n",
    "    v_accs_mlp.append(best[1])\n",
    "    print(\"{}: train acc {} val acc {} random {}\".format(c, best[0], best[1], 1./c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36d113b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "ax.plot(range(2, 6), random_acc, '#FF000080', label=\"random\")\n",
    "ax.plot(range(2, 6), t_accs, '#008000FF', linestyle='--', linewidth=3.0, label=\"train accuracy AdaBoost\")\n",
    "ax.plot(range(2, 6), v_accs, '#0000FFFF', linestyle='--', linewidth=3.0, label=\"validation accuracy AdaBoost\")\n",
    "ax.plot(range(2, 6), t_accs_mlp, '#008000FF', linestyle='-',linewidth=3.0, label=\"train accuracy MLP\")\n",
    "ax.plot(range(2, 6), v_accs_mlp, '#0000FFFF', linestyle='-', linewidth=3.0, label=\"validation accuracy MLP\")\n",
    "ax.set_title('Multi Layer Perceptron vs AdaBoost vs Logistic Regression')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ee89e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictImage:\n",
    "    def __init__(self, image, classifier, width=224, height=224):\n",
    "        self.feature_extractor = FeatureExtractor()\n",
    "        self.classifier = classifier\n",
    "        self.height = height;\n",
    "        self.width = width;\n",
    "        self.image = image;\n",
    "        self.new_image = self.transform(image)\n",
    "        self.features = self.feature_extractor.extract_features(self.new_image)\n",
    "        \n",
    "    def transform(self, image):\n",
    "        height, width, ch = image.shape\n",
    "        if width < height:\n",
    "            new_width = self.width\n",
    "            scale = self.width / width\n",
    "            new_height = int(height * scale)\n",
    "        else:\n",
    "            new_height = self.height\n",
    "            scale = self.height / height\n",
    "            new_width = int(width * scale)\n",
    "        image_tf = transforms.resize(image, (new_height, new_width), 2)\n",
    "        # center crop\n",
    "        w_start = 0\n",
    "        w_stop = self.width\n",
    "        h_start = 0\n",
    "        h_stop = self.height\n",
    "        if new_width > self.width:\n",
    "            start = (new_width - self.width) // 2\n",
    "            w_start = start\n",
    "            w_stop = start + self.width\n",
    "        if new_height > self.height:\n",
    "            start = (new_height - self.height) // 2\n",
    "            h_start = start\n",
    "            h_stop = start + self.height\n",
    "        image_tf = image_tf[h_start:h_stop, w_start:w_stop, :]\n",
    "        return image_tf\n",
    "    \n",
    "    def prediction(self):\n",
    "        fig, ax = plt.subplots(figsize=(8, 4))\n",
    "        ax.imshow(self.image)\n",
    "        ax.axis('off')\n",
    "        plt.tight_layout()\n",
    "        image_features = np.stack(self.features, axis=0)\n",
    "        res = self.classifier.predict(self.image.reshape(-1,1))\n",
    "        return res\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.image, self.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6769ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_filepath = '/Users/cap/Dropbox/Cuisines/images/dan-dan-noodles/dan-dan-1.jpg'\n",
    "image = imageio.imread(image_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c1deb5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ada_classifier, ada_lda = ada_classifiers[0]\n",
    "print(ada_classifier, ada_lda)\n",
    "\n",
    "predictImage = PredictImage(image, ada_classifier)\n",
    "\n",
    "for i, (image, features) in enumerate(predictImage):\n",
    "    features_ada = ada_lda.transform(features.reshape(1, 20))\n",
    "    pred_ada = int(ada_classifier.predict(features_ada.reshape(-1, 1)))\n",
    "    print(pred_ada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a462cc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10dc0803",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
